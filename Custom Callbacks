import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
import os

%matplotlib inline

print('Tensorflow version :',tf.__version__)

(x_train,y_train), (x_test,y_test) = tf.keras.datasets.mnist.load_data()

x_train = np.reshape(x_train, (x_train.shape[0], 28*28)) / 255
x_test = np.reshape(x_test, (x_test.shape[0], 28*28)) / 255

y_train = tf.keras.utils.to_categorical(y_train)
y_test = tf.keras.utils.to_categorical(y_test)

model = Sequential([
    Dense(16, input_shape=(28*28,), activation='relu'),
    Dense(10, activation = 'softmax')
])

opt = tf.keras.optimizers.SGD(learning_rate = 0.02)
model.compile(loss='categorical_crossentropy', optimizer=opt, metrics = ['accuracy'])
model.summary()

#create custom callback class which inherit from keras callbacks.Callback
class CustomCallback(tf.keras.callbacks.Callback):
    def __init__(self, fraction)  #this is the fraction we multiply our learning rate with as the model training goes on with one epoch to another
    '''
    at a high level super() gives you access to methods in a superclass from the subclass that inherits from it,
    super() alone returns a temporary object of the superclass that then allows you to call that superclassâ€™s methods.
