import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from jupyterthemes import jtplot

'''
line below is used for first import the theme 'monokai' and 
it will make xlabel and ylabel much more visible for dark theme of notebook
'''
jtplot.style(theme='monokai', context='notebook', ticks=True, grid=False)

admission_df = pd.read_csv('Admission_Predict.csv')
admission_df.head()

#drop the first column i.e. serial number
admission_df.drop('Serial No.', axis=1, inplace = True)

#check for null values
#is_null() will check all the values and return dataframe with True or False vales
#sum() will add up True values for each column and return the summed up values
admission_df.is_null().sum()

#check the dataframe information gives column names with non-null values and data type for each column
admission_df.info()

#statistical summary of dataframe
admission_df.describe()

#group data by university rating and calculate the mean for all columns values for each rating
df_university = admission_df.groupby('University Rating').mean()
print(df_university)

#plot the histogram
admission_df.hist(bins = 30, figsize = (20,20), color = 'r')

#pairplot with seaborn
sns.pairplot(admission_df)

#plot the correlation matrix with heatmap
corr_matrix = admission_df.corr()
plt.figure(figsize = (12,12))
sns.heatmap(corr_matrix,annot=True)
plt.show()

#take a look at dataframe
admission_df.columns

#create X,y 
X = admission_df.drop(columns=['Chance of Admit'])
y = admission_df['Chance of Admit']

#take a look at shape of X and y
print(X.shape)
print(y.shape)

#convert X and y to numpy array
X = np.array(X)
y = np.array(y)

#change the shape of y from (500,) to (500,1)
y = y.reshape(-1,1)
print(y.shape)

#scale the data before training
from sklearn.preprocessing import StandardScaler, MinMaxScaler
scaler_x = StandardScaler()
X = scaler_x.fit_transform(X)
scaler_y = StandardScaler()
y = scaler_y.fit_transform(y)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15) 

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, accuracy_score

LinearRegression_model = LinearRegression()
LinearRegression_model.fit(X_train,y_train)

accuracy_LinearRegression = LinearRegression_model.score(X_test, y_test)
print(accuracy_LinearRegression)

#train a artificial neural network
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Dense, Dropout, Activation
from tensorflow.keras.optimizers import Adam

ANN_model = keras.Sequential()
ANN_model.add(Dense(50, input_dim = 7))
ANN_model.add(Activation('relu'))

ANN_model.add(Dense(150)
ANN_model.add(Activation('relu'))
ANN_model.add(Dropout(0.5))

ANN_model.add(Dense(150)
ANN_model.add(Activation('relu'))
ANN_model.add(Dropout(0.5))

ANN_model.add(Dense(50)
ANN_model.add(Activation('linear'))
ANN_model.add(Dense(1)

ANN_model.compile(optimizer='adam',loss='mse')

ANN_model.summary()

epochs_hist = ANN_model.fit(X_train,y_train,epoch = 200,batch_size = 20,validation_split = 0.2)
result = ANN_model.evaluate(X_test,y_test)
accuracy_ANN = 1 - result
print('Accuracy: {}'.format(accuracy_ANN))

epochs_hist.history.keys() #to see what are the parameters like loss, val_loss

plt.plot(epochs_hist.history['loss'])
plt.xlabel('Epoch')
plt.ylabel('Training Loss')
plt.title('Model Loss progress during training')
plt.legend(['Training Loss'])

#train a decision tree and random forest
from sklearn.tree import DecisionTreeRegressor
DecisionTree_model = DecisionTreeRegressor()
DecisionTree_model.fit(X_train,y_train)

accuracy_DecisionTree = DecisionTree_model.score(X_test,y_test)
print(accuracy_DecisionTree)

from sklearn.ensemble import RandomForestRegressor
RandomForest_model = RandomForestRegressor(n_estimators = 100, max_depth = 10)
RandomForest_model.fit(X_train,y_train)

accuracy_RandomForest = RandomForest_model.score(X_test,y_test)
print(accuracy_RandomForest)


y_predict = LinearRegression_model.predict(X_test)
plt.plot(y_test,y_predict,'^',color='r')

#transform y_predict, y_test to original scale
y_predict_orig = scaler_y.inverse_transform(y_predict)
y_test_orig = scaler_y.inverse_transform(y_test)
plt.plot(y_test_orig, y_predict_orig, '^', color = 'r')

k = X_test.shape[1] #number of independent variables
n = len(X_test) #number of samples
print(n)

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from math import sqrt

RMSE = float(format(np.sqrt(mean_squared_error(y_test_orig, y_predict_orig)) ,'.3f'))
MSE = mean_squared_error(y_test_orig, y_predict_orig)
MAE = mean_absolute_error(y_test_orig, y_predict_orig)
r2 = r2_score(y_test_orig, y_predict_orig)
adj_r2 = 1 - (1-r2)*(n-1)/(n-k-1)

print('RMSE =',RMSE,'\nMSE =',MSE,'\nMAE =',MAE,'\nR2 =',r2,'\nAdjusted R2 =',adj_r2)
